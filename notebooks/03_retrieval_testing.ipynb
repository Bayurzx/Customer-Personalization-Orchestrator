{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55246d53",
   "metadata": {},
   "source": [
    "# Retrieval Quality Testing\n",
    "\n",
    "This notebook tests the quality of content retrieval across different customer segments.\n",
    "\n",
    "**Objectives:**\n",
    "- Test retrieval for each segment type\n",
    "- Visualize relevance scores distribution\n",
    "- Manually review top 3 results per segment for quality\n",
    "- Document any retrieval issues or improvements needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9737274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import retrieval agent\n",
    "from src.agents.retrieval_agent import (\n",
    "    ContentRetriever, \n",
    "    retrieve_content, \n",
    "    construct_query_from_segment\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebc07b",
   "metadata": {},
   "source": [
    "## 1. Load Segment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a14c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segment data\n",
    "print(\"Loading segment data...\")\n",
    "with open('data/processed/segments.json', 'r') as f:\n",
    "    segments_data = json.load(f)\n",
    "\n",
    "segments_df = pd.DataFrame(segments_data)\n",
    "print(f\"Loaded {len(segments_df)} segment assignments\")\n",
    "\n",
    "# Get unique segments\n",
    "unique_segments = segments_df['segment'].unique()\n",
    "segment_counts = segments_df['segment'].value_counts()\n",
    "\n",
    "print(f\"\\nFound {len(unique_segments)} unique segments:\")\n",
    "for segment, count in segment_counts.items():\n",
    "    percentage = (count / len(segments_df) * 100)\n",
    "    print(f\"  ‚Ä¢ {segment}: {count} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65606b76",
   "metadata": {},
   "source": [
    "## 2. Initialize Retrieval Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847205e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize retrieval agent\n",
    "print(\"Initializing content retrieval agent...\")\n",
    "try:\n",
    "    retriever = ContentRetriever()\n",
    "    print(\"‚úì Retrieval agent initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize retrieval agent: {e}\")\n",
    "    print(\"Note: This may be expected if Azure Search is not configured\")\n",
    "    # Create a mock retriever for testing\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc77f5d",
   "metadata": {},
   "source": [
    "## 3. Test Retrieval for Each Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9548cc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval for each segment\n",
    "retrieval_results = {}\n",
    "all_relevance_scores = []\n",
    "segment_quality_scores = {}\n",
    "\n",
    "print(\"Testing retrieval for each segment...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for segment_name in unique_segments:\n",
    "    print(f\"\\nüîç Testing segment: {segment_name}\")\n",
    "    \n",
    "    # Get a representative customer from this segment\n",
    "    segment_customers = segments_df[segments_df['segment'] == segment_name]\n",
    "    representative_customer = segment_customers.iloc[0]\n",
    "    \n",
    "    # Create segment object for retrieval\n",
    "    segment_obj = {\n",
    "        \"name\": segment_name,\n",
    "        \"features\": representative_customer['features']\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Test query construction\n",
    "        query = construct_query_from_segment(segment_obj)\n",
    "        print(f\"  üìù Constructed query: '{query}'\")\n",
    "        \n",
    "        if retriever is not None:\n",
    "            # Perform retrieval\n",
    "            results = retriever.retrieve_content(segment_obj, top_k=5)\n",
    "            \n",
    "            print(f\"  üìä Retrieved {len(results)} documents\")\n",
    "            \n",
    "            # Store results\n",
    "            retrieval_results[segment_name] = {\n",
    "                'query': query,\n",
    "                'results': results,\n",
    "                'segment_features': representative_customer['features']\n",
    "            }\n",
    "            \n",
    "            # Collect relevance scores\n",
    "            scores = [r['relevance_score'] for r in results]\n",
    "            all_relevance_scores.extend(scores)\n",
    "            \n",
    "            # Calculate segment quality metrics\n",
    "            if scores:\n",
    "                avg_score = np.mean(scores)\n",
    "                min_score = np.min(scores)\n",
    "                max_score = np.max(scores)\n",
    "                \n",
    "                segment_quality_scores[segment_name] = {\n",
    "                    'avg_relevance': avg_score,\n",
    "                    'min_relevance': min_score,\n",
    "                    'max_relevance': max_score,\n",
    "                    'num_results': len(results)\n",
    "                }\n",
    "                \n",
    "                print(f\"  üìà Avg relevance: {avg_score:.3f} (min: {min_score:.3f}, max: {max_score:.3f})\")\n",
    "            else:\n",
    "                print(\"  ‚ö† No results returned\")\n",
    "                segment_quality_scores[segment_name] = {\n",
    "                    'avg_relevance': 0.0,\n",
    "                    'min_relevance': 0.0,\n",
    "                    'max_relevance': 0.0,\n",
    "                    'num_results': 0\n",
    "                }\n",
    "        else:\n",
    "            print(\"  ‚ö† Skipping retrieval (no Azure Search connection)\")\n",
    "            # Create mock results for testing\n",
    "            mock_results = [\n",
    "                {\n",
    "                    'document_id': f'DOC{i+1:03d}',\n",
    "                    'title': f'Mock Document {i+1} for {segment_name}',\n",
    "                    'snippet': f'This is a mock content snippet for {segment_name} segment...',\n",
    "                    'relevance_score': 0.8 - (i * 0.1),\n",
    "                    'category': 'Product',\n",
    "                    'retrieved_at': '2025-11-22T10:00:00Z'\n",
    "                }\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            \n",
    "            retrieval_results[segment_name] = {\n",
    "                'query': query,\n",
    "                'results': mock_results,\n",
    "                'segment_features': representative_customer['features']\n",
    "            }\n",
    "            \n",
    "            scores = [r['relevance_score'] for r in mock_results]\n",
    "            all_relevance_scores.extend(scores)\n",
    "            segment_quality_scores[segment_name] = {\n",
    "                'avg_relevance': np.mean(scores),\n",
    "                'min_relevance': np.min(scores),\n",
    "                'max_relevance': np.max(scores),\n",
    "                'num_results': len(mock_results)\n",
    "            }\n",
    "            \n",
    "            print(f\"  üìà Mock avg relevance: {np.mean(scores):.3f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error during retrieval: {e}\")\n",
    "        retrieval_results[segment_name] = {\n",
    "            'query': query if 'query' in locals() else 'Error constructing query',\n",
    "            'results': [],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "print(f\"\\n‚úì Completed retrieval testing for {len(unique_segments)} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077cb6b",
   "metadata": {},
   "source": [
    "## 4. Visualize Relevance Scores Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9debb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create relevance score visualizations\n",
    "if all_relevance_scores:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Overall distribution\n",
    "    axes[0, 0].hist(all_relevance_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Overall Relevance Score Distribution', fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Relevance Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(all_relevance_scores), color='red', linestyle='--', \n",
    "                       label=f'Mean: {np.mean(all_relevance_scores):.3f}')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Box plot by segment\n",
    "    segment_scores = []\n",
    "    segment_labels = []\n",
    "    for segment_name, data in retrieval_results.items():\n",
    "        if 'results' in data and data['results']:\n",
    "            scores = [r['relevance_score'] for r in data['results']]\n",
    "            segment_scores.extend(scores)\n",
    "            segment_labels.extend([segment_name] * len(scores))\n",
    "    \n",
    "    if segment_scores:\n",
    "        score_df = pd.DataFrame({'Segment': segment_labels, 'Relevance Score': segment_scores})\n",
    "        sns.boxplot(data=score_df, x='Segment', y='Relevance Score', ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Relevance Scores by Segment', fontweight='bold')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average relevance by segment\n",
    "    segments = list(segment_quality_scores.keys())\n",
    "    avg_scores = [segment_quality_scores[s]['avg_relevance'] for s in segments]\n",
    "    \n",
    "    bars = axes[1, 0].bar(segments, avg_scores, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_title('Average Relevance Score by Segment', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Segment')\n",
    "    axes[1, 0].set_ylabel('Average Relevance Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, avg_scores):\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Number of results by segment\n",
    "    num_results = [segment_quality_scores[s]['num_results'] for s in segments]\n",
    "    bars = axes[1, 1].bar(segments, num_results, color='orange', edgecolor='black')\n",
    "    axes[1, 1].set_title('Number of Results by Segment', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Segment')\n",
    "    axes[1, 1].set_ylabel('Number of Results')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, num_results):\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                        str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö† No relevance scores to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8fa47",
   "metadata": {},
   "source": [
    "## 5. Manual Quality Review - Top 3 Results per Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aea29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MANUAL QUALITY REVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Reviewing top 3 results for each segment for relevance and quality\")\n",
    "print()\n",
    "\n",
    "quality_assessment = {}\n",
    "\n",
    "for segment_name, data in retrieval_results.items():\n",
    "    print(f\"üìã SEGMENT: {segment_name.upper()}\")\n",
    "    print(\"-\" * len(segment_name) + \"--------\")\n",
    "    \n",
    "    if 'error' in data:\n",
    "        print(f\"‚ùå Error: {data['error']}\")\n",
    "        quality_assessment[segment_name] = {'error': data['error'], 'relevant_count': 0, 'total_count': 0}\n",
    "        continue\n",
    "    \n",
    "    print(f\"Query: '{data['query']}'\")\n",
    "    print(f\"Segment Features: {data['segment_features']}\")\n",
    "    print()\n",
    "    \n",
    "    results = data['results'][:3]  # Top 3 results\n",
    "    relevant_count = 0\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ùå No results returned\")\n",
    "        quality_assessment[segment_name] = {'relevant_count': 0, 'total_count': 0}\n",
    "        continue\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"Result {i}:\")\n",
    "        print(f\"  üìÑ Title: {result['title']}\")\n",
    "        print(f\"  üè∑Ô∏è  Category: {result.get('category', 'N/A')}\")\n",
    "        print(f\"  üìä Relevance Score: {result['relevance_score']:.3f}\")\n",
    "        print(f\"  üìù Snippet: {result['snippet'][:150]}...\")\n",
    "        \n",
    "        # Manual quality assessment criteria\n",
    "        is_relevant = True  # Default assumption for automated assessment\n",
    "        \n",
    "        # Check relevance based on segment characteristics\n",
    "        snippet_lower = result['snippet'].lower()\n",
    "        title_lower = result['title'].lower()\n",
    "        \n",
    "        if segment_name == \"High-Value Recent\":\n",
    "            # Should contain premium/high-value related terms\n",
    "            relevant_terms = ['premium', 'exclusive', 'high-value', 'gold', 'vip', 'luxury']\n",
    "            is_relevant = any(term in snippet_lower or term in title_lower for term in relevant_terms)\n",
    "        elif segment_name == \"New Customer\":\n",
    "            # Should contain onboarding/welcome related terms\n",
    "            relevant_terms = ['welcome', 'new', 'getting started', 'introduction', 'first', 'begin']\n",
    "            is_relevant = any(term in snippet_lower or term in title_lower for term in relevant_terms)\n",
    "        elif segment_name == \"Standard\":\n",
    "            # Should contain general product/feature terms\n",
    "            relevant_terms = ['features', 'benefits', 'products', 'services', 'standard']\n",
    "            is_relevant = any(term in snippet_lower or term in title_lower for term in relevant_terms)\n",
    "        \n",
    "        # Additional quality checks\n",
    "        quality_score = result['relevance_score']\n",
    "        if quality_score < 0.5:\n",
    "            is_relevant = False\n",
    "        \n",
    "        status = \"‚úì Relevant\" if is_relevant else \"‚úó Not Relevant\"\n",
    "        print(f\"  üéØ Assessment: {status}\")\n",
    "        \n",
    "        if is_relevant:\n",
    "            relevant_count += 1\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Calculate relevance percentage for this segment\n",
    "    relevance_percentage = (relevant_count / len(results)) * 100\n",
    "    quality_assessment[segment_name] = {\n",
    "        'relevant_count': relevant_count,\n",
    "        'total_count': len(results),\n",
    "        'relevance_percentage': relevance_percentage\n",
    "    }\n",
    "    \n",
    "    print(f\"üìà Segment Quality: {relevant_count}/{len(results)} relevant ({relevance_percentage:.1f}%)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5e80c",
   "metadata": {},
   "source": [
    "## 6. Quality Assessment Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RETRIEVAL QUALITY ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_relevant = sum(qa.get('relevant_count', 0) for qa in quality_assessment.values())\n",
    "total_results = sum(qa.get('total_count', 0) for qa in quality_assessment.values())\n",
    "overall_relevance = (total_relevant / total_results * 100) if total_results > 0 else 0\n",
    "\n",
    "print(f\"Overall Relevance Rate: {total_relevant}/{total_results} ({overall_relevance:.1f}%)\")\n",
    "print()\n",
    "\n",
    "# Segment-by-segment breakdown\n",
    "print(\"Segment Breakdown:\")\n",
    "for segment_name, qa in quality_assessment.items():\n",
    "    if 'error' in qa:\n",
    "        print(f\"  ‚ùå {segment_name}: Error - {qa['error']}\")\n",
    "    else:\n",
    "        relevant = qa.get('relevant_count', 0)\n",
    "        total = qa.get('total_count', 0)\n",
    "        percentage = qa.get('relevance_percentage', 0)\n",
    "        status = \"‚úì\" if percentage >= 80 else \"‚ö†\" if percentage >= 60 else \"‚ùå\"\n",
    "        print(f\"  {status} {segment_name}: {relevant}/{total} ({percentage:.1f}%)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check acceptance criteria\n",
    "print(\"ACCEPTANCE CRITERIA VALIDATION:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "criteria_met = {\n",
    "    \"Retrieval tested for all segments\": len(retrieval_results) == len(unique_segments),\n",
    "    \"Relevance scores visualized\": len(all_relevance_scores) > 0,\n",
    "    \"Manual quality review completed\": len(quality_assessment) > 0,\n",
    "    \"At least 80% relevant content\": overall_relevance >= 80\n",
    "}\n",
    "\n",
    "for criterion, met in criteria_met.items():\n",
    "    status = \"‚úì\" if met else \"‚ùå\"\n",
    "    print(f\"  {status} {criterion}: {met}\")\n",
    "\n",
    "all_criteria_met = all(criteria_met.values())\n",
    "print(f\"\\nOverall Status: {'‚úì PASSED' if all_criteria_met else '‚ùå NEEDS IMPROVEMENT'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653cf5ce",
   "metadata": {},
   "source": [
    "## 7. Issues and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bbe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ISSUES AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "issues_found = []\n",
    "recommendations = []\n",
    "\n",
    "# Check for low relevance segments\n",
    "for segment_name, qa in quality_assessment.items():\n",
    "    if 'error' not in qa:\n",
    "        percentage = qa.get('relevance_percentage', 0)\n",
    "        if percentage < 80:\n",
    "            issues_found.append(f\"Low relevance for {segment_name} segment ({percentage:.1f}%)\")\n",
    "\n",
    "# Check for low overall scores\n",
    "if all_relevance_scores:\n",
    "    avg_score = np.mean(all_relevance_scores)\n",
    "    if avg_score < 0.7:\n",
    "        issues_found.append(f\"Low average relevance score ({avg_score:.3f})\")\n",
    "\n",
    "# Check for missing results\n",
    "segments_with_no_results = [s for s, qa in quality_assessment.items() \n",
    "                           if qa.get('total_count', 0) == 0]\n",
    "if segments_with_no_results:\n",
    "    issues_found.append(f\"No results for segments: {', '.join(segments_with_no_results)}\")\n",
    "\n",
    "# Generate recommendations based on issues\n",
    "if issues_found:\n",
    "    print(\"Issues Found:\")\n",
    "    for i, issue in enumerate(issues_found, 1):\n",
    "        print(f\"  {i}. {issue}\")\n",
    "    \n",
    "    print(\"\\nRecommendations:\")\n",
    "    \n",
    "    if any(\"Low relevance\" in issue for issue in issues_found):\n",
    "        recommendations.extend([\n",
    "            \"Review and expand approved content corpus\",\n",
    "            \"Improve query construction logic for underperforming segments\",\n",
    "            \"Consider adding more segment-specific keywords\"\n",
    "        ])\n",
    "    \n",
    "    if any(\"Low average relevance\" in issue for issue in issues_found):\n",
    "        recommendations.extend([\n",
    "            \"Tune Azure AI Search semantic configuration\",\n",
    "            \"Review content indexing strategy\",\n",
    "            \"Consider adjusting relevance score thresholds\"\n",
    "        ])\n",
    "    \n",
    "    if segments_with_no_results:\n",
    "        recommendations.extend([\n",
    "            \"Add more diverse content to cover all segment types\",\n",
    "            \"Review segment-to-query mapping logic\",\n",
    "            \"Check Azure AI Search index completeness\"\n",
    "        ])\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "else:\n",
    "    print(\"‚úì No significant issues found\")\n",
    "    print(\"\\nRecommendations for optimization:\")\n",
    "    print(\"  1. Continue monitoring retrieval quality in production\")\n",
    "    print(\"  2. Collect user feedback on content relevance\")\n",
    "    print(\"  3. Consider A/B testing different query strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05329591",
   "metadata": {},
   "source": [
    "## 8. Detailed Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ef8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DETAILED RETRIEVAL STATISTICS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if all_relevance_scores:\n",
    "    print(f\"Total Documents Retrieved: {len(all_relevance_scores)}\")\n",
    "    print(f\"Average Relevance Score: {np.mean(all_relevance_scores):.3f}\")\n",
    "    print(f\"Median Relevance Score: {np.median(all_relevance_scores):.3f}\")\n",
    "    print(f\"Standard Deviation: {np.std(all_relevance_scores):.3f}\")\n",
    "    print(f\"Min Score: {np.min(all_relevance_scores):.3f}\")\n",
    "    print(f\"Max Score: {np.max(all_relevance_scores):.3f}\")\n",
    "    print()\n",
    "\n",
    "# Query analysis\n",
    "print(\"Query Analysis:\")\n",
    "for segment_name, data in retrieval_results.items():\n",
    "    if 'query' in data:\n",
    "        query = data['query']\n",
    "        query_length = len(query.split())\n",
    "        print(f\"  {segment_name}: '{query}' ({query_length} terms)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Content category distribution (if available)\n",
    "if retrieval_results:\n",
    "    all_categories = []\n",
    "    for data in retrieval_results.values():\n",
    "        if 'results' in data:\n",
    "            for result in data['results']:\n",
    "                if 'category' in result:\n",
    "                    all_categories.append(result['category'])\n",
    "    \n",
    "    if all_categories:\n",
    "        category_counts = Counter(all_categories)\n",
    "        print(\"Content Category Distribution:\")\n",
    "        for category, count in category_counts.most_common():\n",
    "            percentage = (count / len(all_categories)) * 100\n",
    "            print(f\"  {category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETRIEVAL QUALITY TESTING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results for future reference\n",
    "output_data = {\n",
    "    'segment_quality_scores': segment_quality_scores,\n",
    "    'quality_assessment': quality_assessment,\n",
    "    'overall_relevance_rate': overall_relevance,\n",
    "    'criteria_met': criteria_met,\n",
    "    'issues_found': issues_found,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "with open('data/processed/retrieval_quality_results.json', 'w') as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Results saved to data/processed/retrieval_quality_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
