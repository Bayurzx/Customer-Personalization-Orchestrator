{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c8ad59",
   "metadata": {},
   "source": [
    "# Generation Samples Analysis\n",
    "\n",
    "**Task 3.4: Batch Generation Testing**\n",
    "\n",
    "This notebook tests message generation quality across all customer segments and validates the generation pipeline.\n",
    "\n",
    "## Objectives\n",
    "1. Generate variants for each segment type\n",
    "2. Review variant quality manually\n",
    "3. Validate citations are correct\n",
    "4. Calculate token usage and costs\n",
    "5. Document any generation issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029b3b1",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path for imports\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.agents.generation_agent import MessageGenerator, generate_variants\n",
    "from src.agents.retrieval_agent import ContentRetriever, retrieve_content\n",
    "from src.agents.segmentation_agent import load_customer_data\n",
    "from src.integrations.azure_openai import AzureOpenAIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1175575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Imports completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf0170",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load segments, customer data, and set up retrieval for content grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0314c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load segment data\n",
    "print(\"üìä Loading segment data...\")\n",
    "with open('../data/processed/segments.json', 'r') as f:\n",
    "    segments_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df = pd.DataFrame(segments_data)\n",
    "print(f\"Loaded {len(segments_df)} customer segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique segments for testing\n",
    "unique_segments = segments_df.groupby('segment').first().reset_index()\n",
    "print(f\"Found {len(unique_segments)} unique segment types:\")\n",
    "for _, segment in unique_segments.iterrows():\n",
    "    print(f\"  - {segment['segment']}: {segment['features']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load customer data for context\n",
    "print(\"\\nüë• Loading customer data...\")\n",
    "customers_df = load_customer_data('../data/raw/customers.csv')\n",
    "print(f\"Loaded {len(customers_df)} customers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4e08af",
   "metadata": {},
   "source": [
    "## Initialize Generation Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb4637",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Initializing generation components...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd704bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Azure OpenAI client\n",
    "try:\n",
    "    openai_client = AzureOpenAIClient()\n",
    "    print(\"‚úÖ Azure OpenAI client initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Azure OpenAI client initialization failed: {e}\")\n",
    "    print(\"   Using mock responses for testing\")\n",
    "    openai_client = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f6f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize content retriever\n",
    "try:\n",
    "    retriever = ContentRetriever()\n",
    "    print(\"‚úÖ Content retriever initialized\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Content retriever initialization failed: {e}\")\n",
    "    print(\"   Using mock content for testing\")\n",
    "    retriever = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6e1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize message generator\n",
    "generator = MessageGenerator(openai_client)\n",
    "print(\"‚úÖ Message generator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076c6366",
   "metadata": {},
   "source": [
    "## Content Retrieval for Each Segment\n",
    "\n",
    "Retrieve relevant content for each segment to ground the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018c2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Retrieving content for each segment...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_content = {}\n",
    "retrieval_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, segment_row in unique_segments.iterrows():\n",
    "    segment_name = segment_row['segment']\n",
    "    segment_dict = {\n",
    "        'name': segment_name,\n",
    "        'features': segment_row['features']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüìã Retrieving content for: {segment_name}\")\n",
    "    \n",
    "    try:\n",
    "        if retriever:\n",
    "            # Use real retrieval\n",
    "            content = retrieve_content(segment_dict, top_k=5)\n",
    "        else:\n",
    "            # Use mock content for testing\n",
    "            content = [\n",
    "                {\n",
    "                    \"document_id\": f\"DOC00{i}\",\n",
    "                    \"title\": f\"Sample Content {i} for {segment_name}\",\n",
    "                    \"snippet\": f\"This is sample content snippet {i} relevant to {segment_name} customers. It provides valuable information about our products and services tailored to their needs.\",\n",
    "                    \"relevance_score\": 0.9 - (i * 0.1),\n",
    "                    \"paragraph_index\": 0\n",
    "                }\n",
    "                for i in range(1, 4)\n",
    "            ]\n",
    "        \n",
    "        segment_content[segment_name] = content\n",
    "        \n",
    "        # Track retrieval statistics\n",
    "        retrieval_stats.append({\n",
    "            'segment': segment_name,\n",
    "            'content_count': len(content),\n",
    "            'avg_relevance': np.mean([c.get('relevance_score', 0) for c in content]),\n",
    "            'total_snippet_words': sum([len(c.get('snippet', '').split()) for c in content])\n",
    "        })\n",
    "        \n",
    "        print(f\"   Retrieved {len(content)} content pieces\")\n",
    "        print(f\"   Average relevance: {np.mean([c.get('relevance_score', 0) for c in content]):.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Retrieval failed: {e}\")\n",
    "        segment_content[segment_name] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display retrieval statistics\n",
    "retrieval_df = pd.DataFrame(retrieval_stats)\n",
    "print(f\"\\nüìä Content Retrieval Summary:\")\n",
    "print(retrieval_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1615843",
   "metadata": {},
   "source": [
    "## Generate Message Variants\n",
    "\n",
    "Generate 3 variants (urgent, informational, friendly) for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac10d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüéØ Generating message variants for all segments...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84f791",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variants = []\n",
    "generation_stats = []\n",
    "generation_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, segment_row in unique_segments.iterrows():\n",
    "    segment_name = segment_row['segment']\n",
    "    segment_dict = {\n",
    "        'name': segment_name,\n",
    "        'features': segment_row['features']\n",
    "    }\n",
    "    \n",
    "    content = segment_content.get(segment_name, [])\n",
    "    \n",
    "    if not content:\n",
    "        print(f\"‚ö†Ô∏è  Skipping {segment_name} - no content available\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìù Generating variants for: {segment_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Generate all 3 variants for this segment\n",
    "        start_time = datetime.now()\n",
    "        variants = generator.generate_variants(segment_dict, content)\n",
    "        generation_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Track successful variants\n",
    "        for variant in variants:\n",
    "            variant['segment_features'] = segment_dict['features']\n",
    "            all_variants.append(variant)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        total_tokens = sum([v['generation_metadata']['tokens_total'] for v in variants])\n",
    "        total_cost = sum([v['generation_metadata']['cost_usd'] for v in variants])\n",
    "        \n",
    "        generation_stats.append({\n",
    "            'segment': segment_name,\n",
    "            'variants_generated': len(variants),\n",
    "            'generation_time_sec': generation_time,\n",
    "            'total_tokens': total_tokens,\n",
    "            'total_cost_usd': total_cost,\n",
    "            'avg_tokens_per_variant': total_tokens / len(variants) if variants else 0,\n",
    "            'avg_cost_per_variant': total_cost / len(variants) if variants else 0\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated {len(variants)} variants\")\n",
    "        print(f\"   üìä Total tokens: {total_tokens}, Cost: ${total_cost:.4f}\")\n",
    "        \n",
    "        # Display variant summaries\n",
    "        for variant in variants:\n",
    "            validation = variant.get('validation', {})\n",
    "            print(f\"      {variant['tone'].title()}: {variant['variant_id']} \"\n",
    "                  f\"({validation.get('word_count', 0)} words, \"\n",
    "                  f\"{validation.get('citation_count', 0)} citations)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Generation failed for {segment_name}: {str(e)}\"\n",
    "        print(f\"   ‚ùå {error_msg}\")\n",
    "        generation_errors.append({\n",
    "            'segment': segment_name,\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c208557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüéâ Generation completed!\")\n",
    "print(f\"   Total variants generated: {len(all_variants)}\")\n",
    "print(f\"   Segments processed: {len(generation_stats)}\")\n",
    "print(f\"   Errors encountered: {len(generation_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21e145",
   "metadata": {},
   "source": [
    "## Generation Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ebbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generation_stats:\n",
    "    stats_df = pd.DataFrame(generation_stats)\n",
    "    \n",
    "    print(f\"\\nüìä Generation Statistics Summary:\")\n",
    "    print(stats_df.to_string(index=False))\n",
    "    \n",
    "    # Calculate totals\n",
    "    total_variants = stats_df['variants_generated'].sum()\n",
    "    total_tokens = stats_df['total_tokens'].sum()\n",
    "    total_cost = stats_df['total_cost_usd'].sum()\n",
    "    avg_time_per_segment = stats_df['generation_time_sec'].mean()\n",
    "    \n",
    "    print(f\"\\nüí∞ Cost Analysis:\")\n",
    "    print(f\"   Total variants: {total_variants}\")\n",
    "    print(f\"   Total tokens: {total_tokens:,}\")\n",
    "    print(f\"   Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"   Average cost per variant: ${total_cost/total_variants:.4f}\")\n",
    "    print(f\"   Average generation time per segment: {avg_time_per_segment:.2f} seconds\")\n",
    "    \n",
    "    # Visualize generation statistics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Tokens per segment\n",
    "    axes[0, 0].bar(stats_df['segment'], stats_df['total_tokens'])\n",
    "    axes[0, 0].set_title('Total Tokens by Segment')\n",
    "    axes[0, 0].set_ylabel('Tokens')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Cost per segment\n",
    "    axes[0, 1].bar(stats_df['segment'], stats_df['total_cost_usd'])\n",
    "    axes[0, 1].set_title('Total Cost by Segment')\n",
    "    axes[0, 1].set_ylabel('Cost (USD)')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Generation time per segment\n",
    "    axes[1, 0].bar(stats_df['segment'], stats_df['generation_time_sec'])\n",
    "    axes[1, 0].set_title('Generation Time by Segment')\n",
    "    axes[1, 0].set_ylabel('Time (seconds)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average tokens per variant\n",
    "    axes[1, 1].bar(stats_df['segment'], stats_df['avg_tokens_per_variant'])\n",
    "    axes[1, 1].set_title('Average Tokens per Variant')\n",
    "    axes[1, 1].set_ylabel('Tokens per Variant')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60a5b5",
   "metadata": {},
   "source": [
    "## Variant Quality Analysis\n",
    "\n",
    "Analyze the quality of generated variants across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_variants:\n",
    "    print(f\"\\nüîç Analyzing variant quality...\")\n",
    "    \n",
    "    # Create variants DataFrame for analysis\n",
    "    variants_analysis = []\n",
    "    \n",
    "    for variant in all_variants:\n",
    "        validation = variant.get('validation', {})\n",
    "        metadata = variant.get('generation_metadata', {})\n",
    "        \n",
    "        variants_analysis.append({\n",
    "            'variant_id': variant['variant_id'],\n",
    "            'segment': variant['segment'],\n",
    "            'tone': variant['tone'],\n",
    "            'subject_length': len(variant['subject']),\n",
    "            'body_word_count': validation.get('word_count', 0),\n",
    "            'citation_count': validation.get('citation_count', 0),\n",
    "            'is_valid': validation.get('valid', False),\n",
    "            'validation_errors': len(validation.get('errors', [])),\n",
    "            'tokens_used': metadata.get('tokens_total', 0),\n",
    "            'cost_usd': metadata.get('cost_usd', 0.0),\n",
    "            'model': metadata.get('model', 'unknown')\n",
    "        })\n",
    "    \n",
    "    variants_df = pd.DataFrame(variants_analysis)\n",
    "    \n",
    "    print(f\"üìä Variant Quality Summary:\")\n",
    "    print(f\"   Total variants: {len(variants_df)}\")\n",
    "    print(f\"   Valid variants: {variants_df['is_valid'].sum()} ({variants_df['is_valid'].mean()*100:.1f}%)\")\n",
    "    print(f\"   Average subject length: {variants_df['subject_length'].mean():.1f} chars\")\n",
    "    print(f\"   Average body word count: {variants_df['body_word_count'].mean():.1f} words\")\n",
    "    print(f\"   Average citations per variant: {variants_df['citation_count'].mean():.1f}\")\n",
    "    \n",
    "    # Quality by segment\n",
    "    segment_quality = variants_df.groupby('segment').agg({\n",
    "        'is_valid': ['count', 'sum', 'mean'],\n",
    "        'body_word_count': 'mean',\n",
    "        'citation_count': 'mean',\n",
    "        'subject_length': 'mean',\n",
    "        'tokens_used': 'mean',\n",
    "        'cost_usd': 'sum'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\nüìã Quality by Segment:\")\n",
    "    print(segment_quality)\n",
    "    \n",
    "    # Quality by tone\n",
    "    tone_quality = variants_df.groupby('tone').agg({\n",
    "        'is_valid': ['count', 'sum', 'mean'],\n",
    "        'body_word_count': 'mean',\n",
    "        'citation_count': 'mean',\n",
    "        'subject_length': 'mean',\n",
    "        'tokens_used': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    print(f\"\\nüé≠ Quality by Tone:\")\n",
    "    print(tone_quality)\n",
    "    \n",
    "    # Visualize quality metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Validation rate by segment\n",
    "    segment_validation = variants_df.groupby('segment')['is_valid'].mean()\n",
    "    axes[0, 0].bar(segment_validation.index, segment_validation.values)\n",
    "    axes[0, 0].set_title('Validation Rate by Segment')\n",
    "    axes[0, 0].set_ylabel('Validation Rate')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Word count distribution\n",
    "    axes[0, 1].hist(variants_df['body_word_count'], bins=15, alpha=0.7)\n",
    "    axes[0, 1].axvline(150, color='red', linestyle='--', label='Min (150)')\n",
    "    axes[0, 1].axvline(250, color='red', linestyle='--', label='Max (250)')\n",
    "    axes[0, 1].set_title('Body Word Count Distribution')\n",
    "    axes[0, 1].set_xlabel('Word Count')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Citation count distribution\n",
    "    axes[0, 2].hist(variants_df['citation_count'], bins=10, alpha=0.7)\n",
    "    axes[0, 2].set_title('Citation Count Distribution')\n",
    "    axes[0, 2].set_xlabel('Citations per Variant')\n",
    "    \n",
    "    # Subject length by tone\n",
    "    sns.boxplot(data=variants_df, x='tone', y='subject_length', ax=axes[1, 0])\n",
    "    axes[1, 0].axhline(60, color='red', linestyle='--', label='Max (60)')\n",
    "    axes[1, 0].set_title('Subject Length by Tone')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Word count by tone\n",
    "    sns.boxplot(data=variants_df, x='tone', y='body_word_count', ax=axes[1, 1])\n",
    "    axes[1, 1].axhline(150, color='red', linestyle='--', label='Min (150)')\n",
    "    axes[1, 1].axhline(250, color='red', linestyle='--', label='Max (250)')\n",
    "    axes[1, 1].set_title('Body Word Count by Tone')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    # Token usage by segment\n",
    "    sns.boxplot(data=variants_df, x='segment', y='tokens_used', ax=axes[1, 2])\n",
    "    axes[1, 2].set_title('Token Usage by Segment')\n",
    "    axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd6f31",
   "metadata": {},
   "source": [
    "## Citation Validation\n",
    "\n",
    "Validate that citations are correctly extracted and mapped to source content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18376910",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_variants:\n",
    "    print(f\"\\nüìö Validating citations...\")\n",
    "    \n",
    "    citation_analysis = []\n",
    "    citation_errors = []\n",
    "    \n",
    "    for variant in all_variants:\n",
    "        variant_id = variant['variant_id']\n",
    "        segment = variant['segment']\n",
    "        body = variant['body']\n",
    "        citations = variant.get('citations', [])\n",
    "        \n",
    "        # Get source content for this segment\n",
    "        source_content = segment_content.get(segment, [])\n",
    "        source_doc_ids = {doc['document_id'] for doc in source_content}\n",
    "        source_titles = {doc['title'] for doc in source_content}\n",
    "        \n",
    "        # Analyze each citation\n",
    "        for i, citation in enumerate(citations):\n",
    "            doc_id = citation.get('document_id')\n",
    "            title = citation.get('title')\n",
    "            citation_text = citation.get('citation_text', '')\n",
    "            \n",
    "            # Check if citation maps to source content\n",
    "            valid_doc_id = doc_id in source_doc_ids or doc_id == 'unknown'\n",
    "            valid_title = any(title.lower() in src_title.lower() or src_title.lower() in title.lower() \n",
    "                            for src_title in source_titles)\n",
    "            \n",
    "            # Check if citation appears in body\n",
    "            citation_in_body = citation_text in body\n",
    "            \n",
    "            citation_analysis.append({\n",
    "                'variant_id': variant_id,\n",
    "                'segment': segment,\n",
    "                'citation_index': i,\n",
    "                'document_id': doc_id,\n",
    "                'title': title,\n",
    "                'valid_doc_id': valid_doc_id,\n",
    "                'valid_title': valid_title,\n",
    "                'citation_in_body': citation_in_body,\n",
    "                'citation_text': citation_text\n",
    "            })\n",
    "            \n",
    "            # Track errors\n",
    "            if not (valid_doc_id and citation_in_body):\n",
    "                citation_errors.append({\n",
    "                    'variant_id': variant_id,\n",
    "                    'segment': segment,\n",
    "                    'issue': f\"Invalid citation: doc_id={doc_id}, in_body={citation_in_body}\",\n",
    "                    'citation_text': citation_text\n",
    "                })\n",
    "    \n",
    "    if citation_analysis:\n",
    "        citations_df = pd.DataFrame(citation_analysis)\n",
    "        \n",
    "        print(f\"üìä Citation Validation Summary:\")\n",
    "        print(f\"   Total citations: {len(citations_df)}\")\n",
    "        print(f\"   Valid document IDs: {citations_df['valid_doc_id'].sum()} ({citations_df['valid_doc_id'].mean()*100:.1f}%)\")\n",
    "        print(f\"   Valid titles: {citations_df['valid_title'].sum()} ({citations_df['valid_title'].mean()*100:.1f}%)\")\n",
    "        print(f\"   Citations in body: {citations_df['citation_in_body'].sum()} ({citations_df['citation_in_body'].mean()*100:.1f}%)\")\n",
    "        print(f\"   Citation errors: {len(citation_errors)}\")\n",
    "        \n",
    "        # Citation validation by segment\n",
    "        citation_by_segment = citations_df.groupby('segment').agg({\n",
    "            'valid_doc_id': ['count', 'sum', 'mean'],\n",
    "            'valid_title': 'mean',\n",
    "            'citation_in_body': 'mean'\n",
    "        }).round(3)\n",
    "        \n",
    "        print(f\"\\nüìã Citation Validation by Segment:\")\n",
    "        print(citation_by_segment)\n",
    "        \n",
    "        # Show citation errors if any\n",
    "        if citation_errors:\n",
    "            print(f\"\\n‚ö†Ô∏è  Citation Errors ({len(citation_errors)}):\")\n",
    "            for error in citation_errors[:5]:  # Show first 5 errors\n",
    "                print(f\"   {error['variant_id']} ({error['segment']}): {error['issue']}\")\n",
    "            if len(citation_errors) > 5:\n",
    "                print(f\"   ... and {len(citation_errors) - 5} more errors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e9a96b",
   "metadata": {},
   "source": [
    "## Sample Variant Review\n",
    "\n",
    "Display sample variants for manual quality review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad703273",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_variants:\n",
    "    print(f\"\\nüìù Sample Variant Review\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Show one variant per segment and tone combination\n",
    "    for segment_name in variants_df['segment'].unique():\n",
    "        print(f\"\\nüéØ SEGMENT: {segment_name}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        segment_variants = [v for v in all_variants if v['segment'] == segment_name]\n",
    "        \n",
    "        for tone in ['urgent', 'informational', 'friendly']:\n",
    "            tone_variants = [v for v in segment_variants if v['tone'] == tone]\n",
    "            \n",
    "            if tone_variants:\n",
    "                variant = tone_variants[0]  # Take first variant of this tone\n",
    "                validation = variant.get('validation', {})\n",
    "                \n",
    "                print(f\"\\nüìß {tone.upper()} TONE - {variant['variant_id']}\")\n",
    "                print(f\"Subject: {variant['subject']}\")\n",
    "                print(f\"Body ({validation.get('word_count', 0)} words, {validation.get('citation_count', 0)} citations):\")\n",
    "                print(variant['body'][:300] + \"...\" if len(variant['body']) > 300 else variant['body'])\n",
    "                \n",
    "                # Show validation status\n",
    "                if validation.get('valid'):\n",
    "                    print(\"‚úÖ Validation: PASS\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Validation: FAIL - {validation.get('errors', [])}\")\n",
    "                \n",
    "                # Show citations\n",
    "                citations = variant.get('citations', [])\n",
    "                if citations:\n",
    "                    print(f\"Citations ({len(citations)}):\")\n",
    "                    for i, citation in enumerate(citations, 1):\n",
    "                        print(f\"  {i}. [{citation.get('document_id')}] {citation.get('title')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f376f9",
   "metadata": {},
   "source": [
    "## Generation Issues Documentation\n",
    "\n",
    "Document any issues encountered during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e788dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüìã Generation Issues Summary\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation issues\n",
    "if all_variants:\n",
    "    invalid_variants = [v for v in all_variants if not v.get('validation', {}).get('valid', False)]\n",
    "    \n",
    "    if invalid_variants:\n",
    "        print(f\"\\n‚ùå Validation Issues ({len(invalid_variants)} variants):\")\n",
    "        \n",
    "        # Group validation errors\n",
    "        error_counts = Counter()\n",
    "        for variant in invalid_variants:\n",
    "            errors = variant.get('validation', {}).get('errors', [])\n",
    "            for error in errors:\n",
    "                error_counts[error] += 1\n",
    "        \n",
    "        for error, count in error_counts.most_common():\n",
    "            print(f\"   {error}: {count} variants\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ No validation issues - all variants passed validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03561d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation errors\n",
    "if generation_errors:\n",
    "    print(f\"\\n‚ùå Generation Errors ({len(generation_errors)}):\")\n",
    "    for error in generation_errors:\n",
    "        print(f\"   {error['segment']}: {error['error']}\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No generation errors!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citation issues\n",
    "if citation_errors:\n",
    "    print(f\"\\n‚ùå Citation Issues ({len(citation_errors)}):\")\n",
    "    \n",
    "    # Group citation errors by type\n",
    "    citation_error_types = Counter()\n",
    "    for error in citation_errors:\n",
    "        if 'doc_id=' in error['issue']:\n",
    "            citation_error_types['Invalid document ID'] += 1\n",
    "        if 'in_body=' in error['issue']:\n",
    "            citation_error_types['Citation not in body'] += 1\n",
    "    \n",
    "    for error_type, count in citation_error_types.items():\n",
    "        print(f\"   {error_type}: {count} citations\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No citation issues!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcba196",
   "metadata": {},
   "source": [
    "## Cost Projections\n",
    "\n",
    "Project costs for larger scale operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222b193",
   "metadata": {},
   "outputs": [],
   "source": [
    "if generation_stats:\n",
    "    print(f\"\\nüí∞ Cost Projections\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Current costs\n",
    "    total_variants = sum([s['variants_generated'] for s in generation_stats])\n",
    "    total_cost = sum([s['total_cost_usd'] for s in generation_stats])\n",
    "    avg_cost_per_variant = total_cost / total_variants if total_variants > 0 else 0\n",
    "    \n",
    "    print(f\"Current POC:\")\n",
    "    print(f\"   Variants generated: {total_variants}\")\n",
    "    print(f\"   Total cost: ${total_cost:.4f}\")\n",
    "    print(f\"   Cost per variant: ${avg_cost_per_variant:.4f}\")\n",
    "    \n",
    "    # Projections for different scales\n",
    "    scales = [100, 500, 1000, 5000, 10000]\n",
    "    \n",
    "    print(f\"\\nProjections (3 variants per customer):\")\n",
    "    for customers in scales:\n",
    "        projected_variants = customers * 3\n",
    "        projected_cost = projected_variants * avg_cost_per_variant\n",
    "        print(f\"   {customers:,} customers: {projected_variants:,} variants, ${projected_cost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94ab95",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save generation results for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d449ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_variants:\n",
    "    print(f\"\\nüíæ Saving generation results...\")\n",
    "    \n",
    "    # Save all variants\n",
    "    output_file = '../data/processed/generation_samples.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_variants, f, indent=2, default=str)\n",
    "    print(f\"   Saved {len(all_variants)} variants to {output_file}\")\n",
    "    \n",
    "    # Save generation statistics\n",
    "    stats_file = '../data/processed/generation_stats.json'\n",
    "    results_summary = {\n",
    "        'generation_timestamp': datetime.now().isoformat(),\n",
    "        'total_variants': len(all_variants),\n",
    "        'segments_processed': len(generation_stats),\n",
    "        'generation_errors': len(generation_errors),\n",
    "        'citation_errors': len(citation_errors) if citation_analysis else 0,\n",
    "        'total_cost_usd': sum([s['total_cost_usd'] for s in generation_stats]),\n",
    "        'avg_cost_per_variant': avg_cost_per_variant if 'avg_cost_per_variant' in locals() else 0,\n",
    "        'validation_pass_rate': variants_df['is_valid'].mean() if 'variants_df' in locals() else 0,\n",
    "        'generation_stats': generation_stats,\n",
    "        'generation_errors': generation_errors,\n",
    "        'citation_errors': citation_errors if citation_analysis else []\n",
    "    }\n",
    "    \n",
    "    with open(stats_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    print(f\"   Saved generation statistics to {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6da67b",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Summary of generation testing results and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nüéØ Task 3.4 Conclusions\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_variants and generation_stats:\n",
    "    # Calculate key metrics\n",
    "    total_variants = len(all_variants)\n",
    "    total_segments = len(generation_stats)\n",
    "    validation_rate = variants_df['is_valid'].mean() if 'variants_df' in locals() else 0\n",
    "    total_cost = sum([s['total_cost_usd'] for s in generation_stats])\n",
    "    \n",
    "    print(f\"‚úÖ ACCEPTANCE CRITERIA STATUS:\")\n",
    "    print(f\"   ‚úì Variants generated for all segments: {total_segments} segments processed\")\n",
    "    print(f\"   ‚úì Quality review documented: {total_variants} variants analyzed\")\n",
    "    print(f\"   ‚úì Citations verified: {len(citation_analysis) if 'citation_analysis' in locals() else 0} citations checked\")\n",
    "    print(f\"   ‚úì Cost estimates calculated: ${total_cost:.4f} total cost\")\n",
    "    print(f\"   {'‚úì' if len(generation_errors) == 0 else '‚ö†'} No generation errors: {len(generation_errors)} errors encountered\")\n",
    "    \n",
    "    print(f\"\\nüìä KEY METRICS:\")\n",
    "    print(f\"   Validation pass rate: {validation_rate*100:.1f}%\")\n",
    "    print(f\"   Average cost per variant: ${total_cost/total_variants:.4f}\")\n",
    "    print(f\"   Average citations per variant: {variants_df['citation_count'].mean():.1f}\" if 'variants_df' in locals() else \"   Citations: Not calculated\")\n",
    "    \n",
    "    print(f\"\\nüîç QUALITY ASSESSMENT:\")\n",
    "    if validation_rate >= 0.9:\n",
    "        print(f\"   ‚úÖ EXCELLENT: >90% validation pass rate\")\n",
    "    elif validation_rate >= 0.8:\n",
    "        print(f\"   ‚úÖ GOOD: >80% validation pass rate\")\n",
    "    elif validation_rate >= 0.7:\n",
    "        print(f\"   ‚ö†Ô∏è  ACCEPTABLE: >70% validation pass rate\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå NEEDS IMPROVEMENT: <70% validation pass rate\")\n",
    "    \n",
    "    if len(generation_errors) == 0:\n",
    "        print(f\"   ‚úÖ No generation errors encountered\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  {len(generation_errors)} generation errors need investigation\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if validation_rate < 0.9:\n",
    "        print(f\"   ‚Ä¢ Review and improve prompt templates for better validation rates\")\n",
    "    if citation_analysis and len(citation_errors) > 0:\n",
    "        print(f\"   ‚Ä¢ Improve citation extraction regex patterns\")\n",
    "    if total_cost / total_variants > 0.01:\n",
    "        print(f\"   ‚Ä¢ Consider cost optimization for large-scale deployment\")\n",
    "    \n",
    "    print(f\"\\nüéâ Task 3.4 completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7701ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    print(f\"‚ùå Task 3.4 failed - no variants generated\")\n",
    "    print(f\"   Check Azure OpenAI configuration and content retrieval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"Generation Samples Analysis Complete\")\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(f\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
