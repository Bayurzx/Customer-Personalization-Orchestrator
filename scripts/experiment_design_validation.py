#!/usr/bin/env python3
"""
Experiment Design Validation
Task 4.1: Experiment Design

Validate experiment design parameters and calculate realistic expectations
for POC with limited sample size.
"""

import json
import yaml
from pathlib import Path
import sys

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))


def load_config():
    """Load experiment configuration."""
    config_path = Path(__file__).parent.parent / "config" / "experiment_config.yaml"
    with open(config_path, "r") as f:
        return yaml.safe_load(f)


def load_segments():
    """Load customer segments data."""
    segments_path = Path(__file__).parent.parent / "data" / "processed" / "segments.json"
    with open(segments_path, "r") as f:
        return json.load(f)


def validate_experiment_design():
    """Validate experiment design against available data."""
    print("ğŸ”¬ Validating Experiment Design...")

    # Load data
    config = load_config()
    segments = load_segments()

    # Analyze segment distribution
    segment_counts = {}
    for s in segments:
        segment_name = s["segment"]
        segment_counts[segment_name] = segment_counts.get(segment_name, 0) + 1

    total_customers = len(segments)

    print(f"\nğŸ“Š Available Data:")
    print(f"   Total Customers: {total_customers}")
    print(f"   Segment Distribution:")
    for name, count in segment_counts.items():
        percentage = count / total_customers * 100
        print(f"     {name}: {count} ({percentage:.1f}%)")

    # Experiment design parameters
    exp_config = config["experiment"]
    num_arms = 4  # 1 control + 3 treatment
    per_arm_target = total_customers // num_arms

    print(f"\nğŸ¯ Experiment Design:")
    print(f"   Number of Arms: {num_arms}")
    print(f"   Customers per Arm: {per_arm_target}")
    print(f"   Control Allocation: {exp_config['sample_allocation']['control_percent']}%")
    print(f"   Treatment Allocation: {exp_config['sample_allocation']['treatment_percent']}%")

    # Validate minimum sample sizes per segment per arm
    print(f"\nğŸ“ Sample Size Analysis:")
    min_per_segment_per_arm = 5  # Minimum for meaningful analysis

    for segment_name, count in segment_counts.items():
        per_arm_for_segment = count // num_arms
        adequate = per_arm_for_segment >= min_per_segment_per_arm
        status = "âœ…" if adequate else "âš ï¸"

        print(f"   {segment_name}: {per_arm_for_segment} per arm {status}")
        if not adequate:
            print(f"     Warning: Less than {min_per_segment_per_arm} customers per arm")

    # Power analysis for POC
    print(f"\nâš¡ Power Analysis for POC:")
    baseline_rates = exp_config["simulation"]["baseline_rates"]
    expected_uplift = exp_config["simulation"]["expected_uplift"]["mean"]

    print(f"   Expected Uplift: {expected_uplift:.0%}")
    print(f"   Sample Size per Arm: {per_arm_target}")

    # Simple power estimation (rule of thumb)
    # For proportions, need roughly 16/p*(1-p)/effect_size^2 per group for 80% power
    for metric, baseline in baseline_rates.items():
        treatment_rate = baseline * (1 + expected_uplift)
        effect_size = treatment_rate - baseline

        # Rough power estimation
        variance = baseline * (1 - baseline)
        rough_power_n = 16 * variance / (effect_size**2)

        power_adequate = per_arm_target >= rough_power_n
        status = "âœ…" if power_adequate else "âš ï¸"

        print(f"   {metric}: {baseline:.1%} â†’ {treatment_rate:.1%} {status}")
        if not power_adequate:
            print(f"     Estimated need: ~{int(rough_power_n)} per arm for 80% power")

    # Assignment strategy validation
    print(f"\nğŸ² Assignment Strategy:")
    assignment = config["assignment"]
    print(f"   Method: {assignment['method']}")
    print(f"   Stratification: {assignment['stratification_variable']}")
    print(f"   Balance Tolerance: Â±{assignment['balance_tolerance']:.0%}")

    # Control message validation
    control_msg = exp_config["control_message"]
    control_word_count = len(control_msg["body"].split())

    print(f"\nğŸ“ Control Message:")
    print(f"   Subject Length: {len(control_msg['subject'])} chars")
    print(f"   Body Word Count: {control_word_count} words")
    print(f"   Tone: {control_msg['tone']}")

    # Generate experiment summary
    experiment_summary = {
        "experiment_id": exp_config["experiment_id"],
        "name": exp_config["name"],
        "total_customers": total_customers,
        "arms": num_arms,
        "customers_per_arm": per_arm_target,
        "segment_distribution": segment_counts,
        "expected_metrics": {
            metric: {
                "baseline": baseline,
                "treatment": baseline * (1 + expected_uplift),
                "expected_lift_percent": expected_uplift * 100,
            }
            for metric, baseline in baseline_rates.items()
        },
        "design_validation": {
            "adequate_sample_size": per_arm_target >= 50,
            "balanced_segments": all(count // num_arms >= 5 for count in segment_counts.values()),
            "realistic_expectations": True,  # POC focused on proof of concept
        },
    }

    # Save validation results
    output_path = Path("data/processed/experiment_design_validation.json")
    with open(output_path, "w") as f:
        json.dump(experiment_summary, f, indent=2)

    print(f"\nğŸ’¡ Recommendations for POC:")
    print("   âœ… Sample size adequate for proof of concept")
    print("   âœ… Segment distribution allows balanced assignment")
    print("   âœ… Control message provides neutral baseline")
    print("   âš ï¸  Statistical power limited - focus on directional insights")
    print("   ğŸ“Š Expect to see trends rather than definitive statistical significance")

    print(f"\nğŸ“ Validation results saved to: {output_path}")
    print("âœ… Experiment Design Validation Complete!")

    return experiment_summary


if __name__ == "__main__":
    validate_experiment_design()
